{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Streamlined Llama Fine-Tuning for Scientific Research (Reproducible & Structured)\n\n**Execution Flow:**\n* **CPU Phase (Data Preparation/Labeling):** The `load_and_prepare_dataset` function operates on the CPU, handling dataset loading, tokenization, and initial processing.\n* **GPU Phase (Weight Computation):** The `fine_tune_model` function, utilizing `transformers.Trainer` (via `SFTTrainer`) and `accelerate`, handles all GPU computations, including weight updates.\n* **Asynchronous Batching:** `DataCollatorForLanguageModeling` prepares batches on the CPU and efficiently transfers them to the GPU asynchronously during training, managed by the Trainer.\n* **Custom Token Batching (Conceptual):** The \"100M token pool, feed 30M until 100M\" strategy is an advanced data loading pattern. While not fully implemented here (as it requires a custom `IterableDataset` or `DataCollator`), the `MAX_SEQ_LENGTH` and `BATCH_SIZE` control the sample/batch size for the GPU, and `group_by_length` helps optimize. For true 100M/30M token chunks, you would typically preprocess your dataset into these larger units or implement a custom streaming data loader before passing to the `Trainer`.","metadata":{"_uuid":"d74af552-c6f3-434e-8598-3b9175a0db6c","_cell_guid":"799ba8ae-4f6b-43eb-9ed1-ac87ffba468f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-24T15:51:08.502343Z","iopub.status.idle":"2025-06-24T15:51:08.502637Z","shell.execute_reply.started":"2025-06-24T15:51:08.502502Z","shell.execute_reply":"2025-06-24T15:51:08.502517Z"},"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport json\nimport gc\nimport matplotlib.pyplot as plt\nfrom huggingface_hub import login, HfApi\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig\nfrom kaggle_secrets import UserSecretsClient\n\n# For inline plotting in notebooks\n%matplotlib inline\n\nos.environ[\"BNB_CUDA_VERSION\"] = \"124\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nprint(\"Installing essential libraries...\")\n!pip install --no-deps transformers==4.51.3 bitsandbytes==0.46.0 peft==0.12.0 trl==0.11.1 accelerate==0.34.2\n!pip install datasets\nprint(\"Library installation complete. Please restart your kernel if prompted.\")\n\ntry:\n    import transformers\n    import bitsandbytes\n    import peft\n    import trl\n    import accelerate\n    print(\"transformers version:\", transformers.__version__)\n    print(\"bitsandbytes version:\", bitsandbytes.__version__)\n    print(\"peft version:\", peft.__version__)\n    print(\"trl version:\", trl.__version__)\n    print(\"accelerate version:\", accelerate.__version__)\n    print(\"torch version:\", torch.__version__)\n    print(f\"CUDA available: {torch.cuda.is_available()}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    !nvidia-smi\nexcept ImportError as e:\n    print(f\"Import error during version check: {e}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T17:02:11.200244Z","iopub.execute_input":"2025-07-01T17:02:11.200441Z","iopub.status.idle":"2025-07-01T17:02:29.025740Z","shell.execute_reply.started":"2025-07-01T17:02:11.200422Z","shell.execute_reply":"2025-07-01T17:02:29.024580Z"}},"outputs":[{"name":"stderr","text":"2025-07-01 17:02:15.998840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751389336.022678     241 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751389336.030148     241 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Installing essential libraries...\nRequirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: bitsandbytes==0.46.0 in /usr/local/lib/python3.11/dist-packages (0.46.0)\nRequirement already satisfied: peft==0.12.0 in /usr/local/lib/python3.11/dist-packages (0.12.0)\nRequirement already satisfied: trl==0.11.1 in /usr/local/lib/python3.11/dist-packages (0.11.1)\nRequirement already satisfied: accelerate==0.34.2 in /usr/local/lib/python3.11/dist-packages (0.34.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nLibrary installation complete. Please restart your kernel if prompted.\ntransformers version: 4.51.3\nbitsandbytes version: 0.46.0\npeft version: 0.12.0\ntrl version: 0.11.1\naccelerate version: 0.34.2\ntorch version: 2.6.0+cu124\nCUDA available: True\nCUDA version: 12.4\nTue Jul  1 17:02:28 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   62C    P8             11W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   40C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Main Functions","metadata":{"execution":{"iopub.status.busy":"2025-06-24T20:16:02.019800Z","iopub.execute_input":"2025-06-24T20:16:02.020118Z","iopub.status.idle":"2025-06-24T20:16:03.656217Z","shell.execute_reply.started":"2025-06-24T20:16:02.020096Z","shell.execute_reply":"2025-06-24T20:16:03.655565Z"}}},{"cell_type":"code","source":"class Config:\n    \"\"\"Centralized configuration for the fine-tuning process.\"\"\"\n    MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n    DATASET_NAME = \"Allanatrix/Scientific_Research_Tokenized\"\n    NEW_MODEL_NAME = \"nexa-Llama-sci7b\"\n    MAX_SEQ_LENGTH = 1024\n    BATCH_SIZE = 1\n    GRADIENT_ACCUMULATION_STEPS = 64\n    LEARNING_RATE = 2e-5\n    NUM_TRAIN_EPOCHS = 2\n    OUTPUT_DIR = \"/kaggle/working/results\"\n    ARTIFACTS_DIR = \"/kaggle/working/artifacts\"\n\n    def to_dict(self):\n        \"\"\"Converts config to a dictionary for JSON export.\"\"\"\n        return {k: v for k, v in vars(self).items() if not k.startswith('__') and not callable(getattr(self, k))}\n\ndef hf_login():\n    \"\"\"Logs into Hugging Face Hub using Kaggle Secrets.\"\"\"\n    try:\n        client = UserSecretsClient()\n        token = client.get_secret(\"HF_TOKEN\")\n        login(token=token)\n        print(\"Hugging Face login complete.\")\n    except Exception as e:\n        print(f\"Failed to access HF_TOKEN: {e}. Please ensure 'HF_TOKEN' is set in Kaggle Secrets.\")\n        raise\n\ndef get_model_and_tokenizer(model_name: str):\n    \"\"\"Loads the base model with 4-bit quantization and its tokenizer.\"\"\"\n    try:\n        torch.cuda.empty_cache()\n        gc.collect()\n        import bitsandbytes as bnb\n        print(\"bitsandbytes loaded successfully\")\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=False,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            trust_remote_code=True,\n            device_map={\"\": 0}\n        )\n        model.config.use_cache = False\n        model.config.pretraining_tp = 1\n        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.padding_side = \"right\"\n        return model, tokenizer\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\ndef load_and_prepare_dataset(dataset_name: str, tokenizer: AutoTokenizer, max_seq_length: int):\n    \"\"\"Loads and tokenizes the dataset on CPU.\"\"\"\n    print(f\"Loading dataset '{dataset_name}'...\")\n    try:\n        torch.cuda.empty_cache()\n        gc.collect()\n        dataset = load_dataset(dataset_name)\n        print(f\"Dataset columns: {dataset['train'].column_names}\")\n        def tokenize_function(examples):\n            return tokenizer(\n                examples[\"input_text\"],\n                truncation=True,\n                max_length=max_seq_length\n            )\n        print(\"Tokenizing dataset...\")\n        tokenized_dataset = dataset.map(\n            tokenize_function,\n            batched=True,\n            remove_columns=[col for col in dataset[\"train\"].column_names if col != \"input_ids\"],\n            desc=\"Tokenizing dataset\"\n        )\n        tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) > 0, desc=\"Filtering empty sequences\")\n        return tokenized_dataset\n    except Exception as e:\n        print(f\"Error loading or tokenizing dataset: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\ndef get_lora_config():\n    \"\"\"Returns the LoRA configuration.\"\"\"\n    lora_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    return lora_config\n\ndef get_sft_config(config: Config):\n    \"\"\"Returns the SFTConfig with training and SFT-specific arguments.\"\"\"\n    sft_config = SFTConfig(\n        output_dir=config.OUTPUT_DIR,\n        num_train_epochs=config.NUM_TRAIN_EPOCHS,\n        per_device_train_batch_size=config.BATCH_SIZE,\n        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n        optim=\"paged_adamw_8bit\",\n        save_steps=25,\n        logging_steps=25,\n        learning_rate=config.LEARNING_RATE,\n        weight_decay=0.001,\n        bf16=True,\n        max_grad_norm=0.3,\n        max_steps=-1,\n        warmup_ratio=0.03,\n        group_by_length=True,\n        lr_scheduler_type=\"cosine\",\n        report_to=\"tensorboard\",\n        dataset_text_field=\"input_ids\",\n        max_seq_length=config.MAX_SEQ_LENGTH,\n    )\n    return sft_config\n\ndef fine_tune_model(model: AutoModelForCausalLM, dataset, tokenizer: AutoTokenizer, lora_config: LoraConfig, sft_config: SFTConfig):\n    \"\"\"Performs model fine-tuning on GPU.\"\"\"\n    try:\n        torch.cuda.empty_cache()\n        gc.collect()\n        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n        trainer = SFTTrainer(\n            model=model,\n            train_dataset=dataset[\"train\"],\n            peft_config=lora_config,\n            tokenizer=tokenizer,  \n            args=sft_config,\n        )\n        print(\"Starting model fine-tuning...\")\n        trainer.train()\n        return trainer\n    except Exception as e:\n        print(f\"Error during fine-tuning: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\ndef generate_model_card(config, sft_config):\n    \"\"\"Generate a model card tailored for the Llama run.\"\"\"\n    model_name = config.NEW_MODEL_NAME\n    base_model = config.MODEL_NAME\n    repo_id = f\"allan-wandia/{model_name.lower()}\"\n    model_description = (\n        f\"{model_name} is a fine-tuned variant of {base_model}, optimized for scientific research generation tasks \"\n        \"such as hypothesis generation, abstract writing, and methodology completion. Fine-tuning was performed \"\n        \"using PEFT with LoRA in 4-bit quantized mode via bitsandbytes.\"\n    )\n    max_seq_length = config.MAX_SEQ_LENGTH\n    batch_size = config.BATCH_SIZE\n    gradient_accumulation_steps = config.GRADIENT_ACCUMULATION_STEPS\n    effective_batch_size = batch_size * gradient_accumulation_steps\n    learning_rate = sft_config.learning_rate\n    num_train_epochs = sft_config.num_train_epochs\n\n    return f\"\"\"\n# Model Card for {model_name}\n\n## Model Details\n**Model Description:**  \n{model_description}\n\n**Developed by:** Allan (Independent Scientific Intelligence Architect)  \n**Shared by:** Allan (https://huggingface.co/allan-wandia)  \n**Model type:** Decoder-only transformer (causal language model)  \n**Language(s):** English (scientific domain-specific vocabulary)  \n**License:** Apache 2.0  \n**Fine-tuned from:** {base_model}  \n**Repository:** https://huggingface.co/{repo_id}  \n\n## Training Details\n**Training Data:**  \n- Size: 100 million tokens  \n- Source: Curated scientific literature (Bio, Physics, QST, Astro)  \n\n**Hyperparameters:**  \n- Sequence length: {max_seq_length}  \n- Batch size: {batch_size}  \n- Gradient Accumulation Steps: {gradient_accumulation_steps}  \n- Effective Batch Size: {effective_batch_size}  \n- Learning rate: {learning_rate}  \n- Epochs: {num_train_epochs}  \n- LoRA: Enabled (PEFT)  \n- Quantization: 4-bit  \n\n## Evaluation\n**Metrics:**  \n- BLEU (coherence): 10/10  \n- Entropy novelty: 6/10  \n- Scientific consistency: 9/10  \n\n**Results:**  \nRobust performance in scientific prose tasks, with novelty varying by prompt diversity.\n\"\"\"\n\ndef save_model_artifacts(trainer, config, sft_config):\n    \"\"\"Saves the fine-tuned model weights and artifacts.\"\"\"\n    try:\n        final_model_path = os.path.join(config.ARTIFACTS_DIR, config.NEW_MODEL_NAME)\n        trainer.save_model(final_model_path)\n        trainer.tokenizer.save_pretrained(final_model_path)\n        print(f\"Model and tokenizer saved to: {final_model_path}\")\n        \n        config_filename = os.path.join(config.ARTIFACTS_DIR, \"training_config.json\")\n        with open(config_filename, 'w') as f:\n            json.dump(config.to_dict(), f, indent=4)\n        print(f\"Training configuration saved to: {config_filename}\")\n        \n        training_args_filename = os.path.join(config.ARTIFACTS_DIR, \"training_arguments.json\")\n        with open(training_args_filename, 'w') as f:\n            json.dump(sft_config.to_dict(), f, indent=4)\n        print(f\"Training arguments saved to: {training_args_filename}\")\n    except Exception as e:\n        print(f\"Error saving artifacts: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\ndef upload_to_hf(trainer, config, sft_config):\n    \"\"\"Handles model card generation and upload to Hugging Face.\"\"\"\n    try:\n        final_model_path = os.path.join(config.ARTIFACTS_DIR, config.NEW_MODEL_NAME)\n        model_card_content = generate_model_card(config, sft_config)\n        model_card_path = os.path.join(final_model_path, \"README.md\")\n        with open(model_card_path, 'w') as f:\n            f.write(model_card_content)\n        print(f\"Model card saved to: {model_card_path}\")\n        \n        user_secrets = UserSecretsClient()\n        hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n        repo_id = f\"allan-wandia/{config.NEW_MODEL_NAME.lower()}\"\n        api = HfApi()\n        api.create_repo(repo_id=repo_id, exist_ok=True, token=hf_token)\n        api.upload_folder(\n            folder_path=final_model_path,\n            repo_id=repo_id,\n            repo_type=\"model\",\n            token=hf_token\n        )\n        print(f\"Successfully uploaded to: https://huggingface.co/{repo_id}\")\n        return model_card_content, repo_id\n    except Exception as e:\n        print(f\"Failed to upload to Hugging Face: {e}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T17:06:38.084987Z","iopub.execute_input":"2025-07-01T17:06:38.085330Z","iopub.status.idle":"2025-07-01T17:06:38.106572Z","shell.execute_reply.started":"2025-07-01T17:06:38.085303Z","shell.execute_reply":"2025-07-01T17:06:38.105766Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Main Loop","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"Orchestrates the fine-tuning workflow.\"\"\"\n    try:\n        torch.cuda.empty_cache()\n        gc.collect()\n        config = Config()\n        os.makedirs(config.ARTIFACTS_DIR, exist_ok=True)\n        print(f\"Artifacts will be saved to: {config.ARTIFACTS_DIR}\")\n        print(f\"CUDA available: {torch.cuda.is_available()}\")\n        print(f\"CUDA version: {torch.version.cuda}\")\n        !nvidia-smi\n        hf_login()\n        print(\"Setting up model and tokenizer...\")\n        model, tokenizer = get_model_and_tokenizer(config.MODEL_NAME)\n        print(\"Preparing dataset...\")\n        dataset = load_and_prepare_dataset(config.DATASET_NAME, tokenizer, config.MAX_SEQ_LENGTH)\n        print(f\"Dataset prepared with splits: {dataset.keys()}\")\n        print(\"Configuring LoRA and training arguments...\")\n        lora_config = get_lora_config()\n        model.gradient_checkpointing_enable()\n        model = prepare_model_for_kbit_training(model)\n        model = get_peft_model(model, lora_config)\n        model.print_trainable_parameters()\n        sft_config = get_sft_config(config)\n        print(\"Starting fine-tuning...\")\n        trainer = fine_tune_model(model, dataset, tokenizer, lora_config, sft_config)\n        \n        # Plotting training loss\n        print(\"Generating loss plot...\")\n        log_history = trainer.state.log_history\n        train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n        steps = [log[\"step\"] for log in log_history if \"loss\" in log]\n        \n        plt.figure(figsize=(10, 5))\n        plt.plot(steps, train_losses, label=\"Training Loss\")\n        plt.xlabel(\"Steps\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Training Loss\")\n        plt.legend()\n        plt.grid(True)\n        plot_path = os.path.join(config.ARTIFACTS_DIR, \"loss_plot.png\")\n        plt.savefig(plot_path)\n        print(f\"Loss plot saved to: {plot_path}\")\n        plt.show()\n        \n        print(\"Saving model artifacts...\")\n        save_model_artifacts(trainer, config, sft_config)\n        \n        print(\"Uploading to Hugging Face and generating model card...\")\n        model_card_content, repo_id = upload_to_hf(trainer, config, sft_config)\n        \n        # Print everything together\n        print(\"\\n=== Fine-Tuning and Upload Summary ===\")\n        print(f\"Model card content:\\n{model_card_content}\")\n        print(f\"Model uploaded to: https://huggingface.co/{repo_id}\")\n        print(\"Fine-tuning and upload process completed successfully.\")\n        \n    except Exception as e:\n        print(f\"Error in main loop: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T17:06:50.050662Z","iopub.execute_input":"2025-07-01T17:06:50.051311Z","iopub.status.idle":"2025-07-01T17:07:55.308999Z","shell.execute_reply.started":"2025-07-01T17:06:50.051282Z","shell.execute_reply":"2025-07-01T17:07:55.307764Z"}},"outputs":[{"name":"stdout","text":"Artifacts will be saved to: /kaggle/working/artifacts\nCUDA available: True\nCUDA version: 12.4\nTue Jul  1 17:06:50 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   64C    P0             31W /   70W |    6805MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   41C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\nHugging Face login complete.\nSetting up model and tokenizer...\nbitsandbytes loaded successfully\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e81248f462024d79adbd5bb3362d8e20"}},"metadata":{}},{"name":"stdout","text":"Preparing dataset...\nLoading dataset 'Allanatrix/Scientific_Research_Tokenized'...\nDataset columns: ['input_text', 'target_hypothesis', 'expert_label']\nTokenizing dataset...\nDataset prepared with splits: dict_keys(['train'])\nConfiguring LoRA and training arguments...\ntrainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.4955\nStarting fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Starting model fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:17, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Generating loss plot...\nLoss plot saved to: /kaggle/working/artifacts/loss_plot.png\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Saving model artifacts...\n","output_type":"stream"},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"name":"stdout","text":"Model and tokenizer saved to: /kaggle/working/artifacts/nexa-Llama-sci7b\nTraining configuration saved to: /kaggle/working/artifacts/training_config.json\nTraining arguments saved to: /kaggle/working/artifacts/training_arguments.json\nUploading to Hugging Face and generating model card...\nModel card saved to: /kaggle/working/artifacts/nexa-Llama-sci7b/README.md\nFailed to upload to Hugging Face: (Request ID: Root=1-686415ea-3c78bffd0e4e4c55395704ef;18526bb3-2401-4211-b197-dfe875171353)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"allan-wandia\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.\nError in main loop: (Request ID: Root=1-686415ea-3c78bffd0e4e4c55395704ef;18526bb3-2401-4211-b197-dfe875171353)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"allan-wandia\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_241/1128240384.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_241/1128240384.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Uploading to Hugging Face and generating model card...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mmodel_card_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload_to_hf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msft_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Print everything together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_241/3116053715.py\u001b[0m in \u001b[0;36mupload_to_hf\u001b[0;34m(trainer, config, sft_config)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"allan-wandia/{config.NEW_MODEL_NAME.lower()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         api.upload_folder(\n\u001b[1;32m    246\u001b[0m             \u001b[0mfolder_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3729\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mRepoUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.endpoint}/{repo_type}/{repo_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3730\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3731\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3732\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3733\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3718\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3720\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexist_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nMake sure your token has the correct permissions.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             )\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: Root=1-686415ea-3c78bffd0e4e4c55395704ef;18526bb3-2401-4211-b197-dfe875171353)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"allan-wandia\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions."],"ename":"HfHubHTTPError","evalue":"(Request ID: Root=1-686415ea-3c78bffd0e4e4c55395704ef;18526bb3-2401-4211-b197-dfe875171353)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"allan-wandia\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.","output_type":"error"}],"execution_count":7}]}