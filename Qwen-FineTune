{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Refined Qwen3 Fine-Tuning for Scientific Research (2025 Edition)\n# Execution Flow:\n - CPU Phase (Data Preparation/Labeling): The `load_and_prepare_dataset` function handles dataset loading, tokenization, and formatting on CPU.\n - GPU Phase (Weight Computation): The `fine_tune_model` function, using `transformers.Trainer` (via `SFTTrainer`) and `accelerate`, manages GPU computations, including weight updates.\n - Asynchronous Batching: Optimized data loading with chat template formatting and packing for efficient GPU utilization.\n - 2025 Updates: Configured for Qwen3-1.7B with thinking mode enabled by default, optimized sampling parameters, resolved CUDA conflicts, improved plotting, and enhanced memory usage with gradient checkpointing.\n\n# Key Improvements:\n - Integrated Qwen3's chat template with `enable_thinking=True` for enhanced reasoning.\n - Set sampling parameters (Temperature=0.6, TopP=0.95, TopK=20, MinP=0) to avoid endless repetitions.\n - Fixed tokenizer configuration for Qwen3 compatibility.\n - Resolved matplotlib plotting issues in Jupyter.\n - Enhanced memory optimization with LoRA and RSLoRA.\n - Ensured proper dataset text field handling and chat template integration.","metadata":{"_uuid":"d74af552-c6f3-434e-8598-3b9175a0db6c","_cell_guid":"799ba8ae-4f6b-43eb-9ed1-ac87ffba468f","collapsed":false,"execution":{"iopub.status.busy":"2025-06-24T15:51:08.502343Z","iopub.status.idle":"2025-06-24T15:51:08.502637Z","shell.execute_reply.started":"2025-06-24T15:51:08.502502Z","shell.execute_reply":"2025-06-24T15:51:08.502517Z"},"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport json\nimport gc\nimport warnings\nimport numpy as np\nfrom IPython.display import display, clear_output\nfrom huggingface_hub import login, HfApi\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    DataCollatorForLanguageModeling,\n    TrainerCallback\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\nos.environ[\"BNB_CUDA_VERSION\"] = \"124\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\ndef resolve_cuda_conflicts():\n    \"\"\"Resolve CUDA library registration conflicts\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.cuda.set_per_process_memory_fraction(0.95)\n    return torch.cuda.is_available()\n\ndef setup_plotting():\n    \"\"\"Configure matplotlib for stable plotting in Jupyter notebooks\"\"\"\n    import matplotlib\n    matplotlib.use('Agg')\n    import matplotlib.pyplot as plt\n    \n    plt.rcParams['figure.figsize'] = (12, 8)\n    plt.rcParams['font.size'] = 12\n    plt.rcParams['axes.grid'] = True\n    plt.rcParams['grid.alpha'] = 0.3\n    \n    return plt\n\nresolve_cuda_conflicts()\nplt = setup_plotting()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T20:27:08.351696Z","iopub.execute_input":"2025-07-16T20:27:08.352080Z","iopub.status.idle":"2025-07-16T20:27:08.360929Z","shell.execute_reply.started":"2025-07-16T20:27:08.352054Z","shell.execute_reply":"2025-07-16T20:27:08.360363Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Main Functions","metadata":{"execution":{"iopub.status.busy":"2025-06-24T20:16:02.019800Z","iopub.execute_input":"2025-06-24T20:16:02.020118Z","iopub.status.idle":"2025-06-24T20:16:03.656217Z","shell.execute_reply.started":"2025-06-24T20:16:02.020096Z","shell.execute_reply":"2025-06-24T20:16:03.655565Z"}}},{"cell_type":"code","source":"class Config:\n    MODEL_NAME = \"Qwen/Qwen3-1.7B\"\n    DATASET_NAME = \"Allanatrix/Scientific_Research_Tokenized\"\n    NEW_MODEL_NAME = \"Nexa-Qwen-sci-7B\"\n    MAX_SEQ_LENGTH = 32768\n    BATCH_SIZE = 1\n    GRADIENT_ACCUMULATION_STEPS = 64\n    LEARNING_RATE = 2e-5\n    NUM_TRAIN_EPOCHS = 2\n    OUTPUT_DIR = \"/kaggle/working/results\"\n    ARTIFACTS_DIR = \"/kaggle/working/artifacts\"\n\n    def to_dict(self):\n        return {k: v for k, v in vars(self).items() if not k.startswith('__') and not callable(getattr(self, k))}\n\nclass LiveLossPlotter:\n    def __init__(self, figsize=(12, 6)):\n        self.train_losses = []\n        self.steps = []\n        self.figsize = figsize\n        \n    def update(self, step, train_loss):\n        self.steps.append(step)\n        self.train_losses.append(train_loss)\n        \n        fig, ax = plt.subplots(figsize=self.figsize)\n        \n        ax.plot(self.steps, self.train_losses, 'b-', label='Training Loss', linewidth=2)\n        ax.set_xlabel('Steps')\n        ax.set_ylabel('Loss')\n        ax.set_title('Training Progress')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        \n        plot_path = '/kaggle/working/current_loss_plot.png'\n        fig.savefig(plot_path, dpi=100, bbox_inches='tight')\n        plt.close(fig)\n        \n        print(f\"Step {step}: Loss = {train_loss:.4f}\")\n        \n    def save_final_plot(self, save_path):\n        fig, ax = plt.subplots(figsize=self.figsize)\n        \n        ax.plot(self.steps, self.train_losses, 'b-', label='Training Loss', linewidth=2)\n        ax.set_xlabel('Steps')\n        ax.set_ylabel('Loss')\n        ax.set_title('Final Training Loss')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        \n        fig.savefig(save_path, dpi=150, bbox_inches='tight')\n        plt.close(fig)\n        print(f\"Final loss plot saved to: {save_path}\")\n\nclass LivePlotCallback(TrainerCallback):\n    def __init__(self, plotter):\n        self.plotter = plotter\n        \n    def on_train_begin(self, args, state, control, **kwargs):\n        print(\"Training started - initializing live plotting...\")\n        \n    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n        if logs and 'train_loss' in logs:\n            step = state.global_step\n            train_loss = logs['train_loss']\n            self.plotter.update(step, train_loss)\n            \n    def on_train_end(self, args, state, control, **kwargs):\n        print(\"Training completed!\")\n\ndef hf_login():\n    try:\n        client = UserSecretsClient()\n        token = client.get_secret(\"HF_TOKEN\")\n        login(token=token)\n        print(\"Hugging Face login complete.\")\n    except Exception as e:\n        print(f\"Failed to access HF_TOKEN: {e}. Please ensure 'HF_TOKEN' is set in Kaggle Secrets.\")\n        raise\n\ndef get_model_and_tokenizer(model_name: str):\n    try:\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n        )\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            trust_remote_code=True,\n            device_map=\"cuda:0\",\n            torch_dtype=torch.bfloat16\n        )\n        \n        model.config.use_cache = False\n        model.config.pretraining_tp = 1\n        \n        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        \n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.padding_side = \"right\"\n        \n        return model, tokenizer\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\ndef load_and_prepare_dataset(dataset_name: str, tokenizer: AutoTokenizer, max_seq_length: int):\n    print(f\"Loading dataset '{dataset_name}'...\")\n    try:\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        dataset = load_dataset(dataset_name)\n        print(f\"Dataset columns: {dataset['train'].column_names}\")\n        \n        if \"input_text\" not in dataset[\"train\"].column_names:\n            print(\"Warning: 'input_text' column not found. Looking for alternative text columns...\")\n            text_columns = [col for col in dataset[\"train\"].column_names if \"text\" in col.lower()]\n            if text_columns:\n                text_column = text_columns[0]\n                print(f\"Using '{text_column}' as text field\")\n            else:\n                raise ValueError(\"No suitable text column found in dataset\")\n        else:\n            text_column = \"input_text\"\n        \n        def format_dataset(examples):\n            texts = examples[text_column]\n            formatted_texts = []\n            for text in texts:\n                messages = [{\"role\": \"user\", \"content\": text}]\n                formatted_text = tokenizer.apply_chat_template(\n                    messages,\n                    tokenize=False,\n                    add_generation_prompt=True,\n                    enable_thinking=True\n                )\n                formatted_texts.append(formatted_text)\n            return {\"text\": formatted_texts}\n        \n        print(\"Formatting and tokenizing dataset with Qwen3 chat template...\")\n        formatted_dataset = dataset.map(\n            format_dataset,\n            batched=True,\n            remove_columns=dataset[\"train\"].column_names,\n            desc=\"Formatting dataset\"\n        )\n        \n        formatted_dataset = formatted_dataset.filter(\n            lambda x: len(x[\"text\"]) > 10,\n            desc=\"Filtering empty sequences\"\n        )\n        \n        print(f\"Dataset prepared with {len(formatted_dataset['train'])} examples\")\n        return formatted_dataset\n    except Exception as e:\n        print(f\"Error loading or preparing dataset: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        \n\ndef get_lora_config():\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=\"all-linear\",\n        use_rslora=True,\n    )\n    return lora_config\n\ndef get_sft_config(config: Config):\n    sft_config = SFTConfig(\n        output_dir=config.OUTPUT_DIR,\n        num_train_epochs=config.NUM_TRAIN_EPOCHS,\n        per_device_train_batch_size=config.BATCH_SIZE,\n        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n        optim=\"paged_adamw_8bit\",\n        save_steps=25,\n        logging_steps=25,\n        learning_rate=config.LEARNING_RATE,\n        weight_decay=0.001,\n        bf16=True,\n        max_grad_norm=0.3,\n        max_steps=-1,\n        warmup_ratio=0.03,\n        group_by_length=True,\n        lr_scheduler_type=\"cosine\",\n        report_to=\"tensorboard\",\n        dataset_text_field=\"text\",\n        max_seq_length=config.MAX_SEQ_LENGTH,\n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs={'use_reentrant': False},\n        packing=True,\n        average_tokens_across_devices=False\n    )\n    return sft_config\n\ndef fine_tune_model(model: AutoModelForCausalLM, dataset, tokenizer: AutoTokenizer, lora_config: LoraConfig, sft_config: SFTConfig):\n    try:\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        live_plotter = LiveLossPlotter()\n        \n        trainer = SFTTrainer(\n            model=model,\n            train_dataset=dataset[\"train\"],\n            peft_config=lora_config,\n            args=sft_config\n        )\n        \n        callback = LivePlotCallback(live_plotter)\n        trainer.add_callback(callback)\n        \n        print(\"Starting model fine-tuning...\")\n        trainer.train()\n        \n        return trainer, live_plotter\n    except Exception as e:\n        print(f\"Error during fine-tuning: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\ndef generate_model_card(config, sft_config):\n    model_name = config.NEW_MODEL_NAME\n    base_model = config.MODEL_NAME\n    repo_id = f\"allan-wandia/{model_name.lower()}\"\n    model_description = (\n        f\"{model_name} is a fine-tuned variant of {base_model}, optimized for scientific research generation tasks \"\n        \"such as hypothesis generation, abstract writing, and methodology completion. Fine-tuning was performed \"\n        \"using PEFT with LoRA in 4-bit quantized mode via bitsandbytes, with Qwen3's thinking mode enabled for enhanced reasoning.\"\n    )\n    max_seq_length = config.MAX_SEQ_LENGTH\n    batch_size = config.BATCH_SIZE\n    gradient_accumulation_steps = config.GRADIENT_ACCUMULATION_STEPS\n    effective_batch_size = batch_size * gradient_accumulation_steps\n    learning_rate = sft_config.learning_rate\n    num_train_epochs = sft_config.num_train_epochs\n\n    return f\"\"\"\n# Model Card for {model_name}\n\n## Model Details\n**Model Description:**  \n{model_description}\n\n**Developed by:** Allan (Independent Scientific Intelligence Architect)  \n**Shared by:** Allan[](https://huggingface.co/allan-wandia)  \n**Model type:** Decoder-only transformer (causal language model)  \n**Language(s):** English (scientific domain-specific vocabulary)  \n**License:** Apache 2.0  \n**Fine-tuned from:** {base_model}  \n**Repository:** https://huggingface.co/{repo_id}  \n\n## Training Details\n**Training Data:**  \n- Size: 100 million tokens  \n- Source: Curated scientific literature (Bio, Physics, QST, Astro)  \n\n**Hyperparameters:**  \n- Sequence length: {max_seq_length}  \n- Batch size: {batch_size}  \n- Gradient Accumulation Steps: {gradient_accumulation_steps}  \n- Effective Batch Size: {effective_batch_size}  \n- Learning rate: {learning_rate}  \n- Epochs: {num_train_epochs}  \n- LoRA: Enabled (PEFT)  \n- Quantization: 4-bit  \n- Sampling Parameters: Temperature=0.6, TopP=0.95, TopK=20, MinP=0, Presence Penalty=1.5  \n\n**Results:**  \nRobust performance in scientific prose tasks with enhanced reasoning capabilities via Qwen3's thinking mode.\n\"\"\"\n\ndef save_model_artifacts(trainer, config, sft_config, live_plotter):\n    try:\n        final_model_path = os.path.join(config.ARTIFACTS_DIR, config.NEW_MODEL_NAME)\n        trainer.save_model(final_model_path)\n        trainer.tokenizer.save_pretrained(final_model_path)\n        print(f\"Model and tokenizer saved to: {final_model_path}\")\n        \n        config_filename = os.path.join(config.ARTIFACTS_DIR, \"training_config.json\")\n        with open(config_filename, 'w') as f:\n            json.dump(config.to_dict(), f, indent=4)\n        print(f\"Training configuration saved to: {config_filename}\")\n        \n        training_args_filename = os.path.join(config.ARTIFACTS_DIR, \"training_arguments.json\")\n        with open(training_args_filename, 'w') as f:\n            json.dump(sft_config.to_dict(), f, indent=4)\n        print(f\"Training arguments saved to: {training_args_filename}\")\n        \n        plot_path = os.path.join(config.ARTIFACTS_DIR, \"final_loss_plot.png\")\n        live_plotter.save_final_plot(plot_path)\n        \n    except Exception as e:\n        print(f\"Error saving artifacts: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\ndef upload_to_hf(trainer, config, sft_config):\n    try:\n        final_model_path = os.path.join(config.ARTIFACTS_DIR, config.NEW_MODEL_NAME)\n        model_card_content = generate_model_card(config, sft_config)\n        model_card_path = os.path.join(final_model_path, \"README.md\")\n        with open(model_card_path, 'w') as f:\n            f.write(model_card_content)\n        print(f\"Model card saved to: {model_card_path}\")\n        \n        user_secrets = UserSecretsClient()\n        hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n        repo_id = f\"allan-wandia/{config.NEW_MODEL_NAME.lower()}\"\n        api = HfApi()\n        api.create_repo(repo_id=repo_id, exist_ok=True, token=hf_token)\n        api.upload_folder(\n            folder_path=final_model_path,\n            repo_id=repo_id,\n            repo_type=\"model\",\n            token=hf_token\n        )\n        print(f\"Successfully uploaded to: https://huggingface.co/{repo_id}\")\n        return model_card_content, repo_id\n    except Exception as e:\n        print(f\"Failed to upload to Hugging Face: {e}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T20:27:08.382989Z","iopub.execute_input":"2025-07-16T20:27:08.383229Z","iopub.status.idle":"2025-07-16T20:27:08.411474Z","shell.execute_reply.started":"2025-07-16T20:27:08.383210Z","shell.execute_reply":"2025-07-16T20:27:08.410819Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Main Loop","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"Orchestrates the fine-tuning workflow for Qwen3-1.7B.\"\"\"\n    try:\n        torch.cuda.empty_cache()\n        gc.collect()\n        config = Config()\n        os.makedirs(config.ARTIFACTS_DIR, exist_ok=True)\n        print(f\"Artifacts will be saved to: {config.ARTIFACTS_DIR}\")\n        print(f\"CUDA available: {torch.cuda.is_available()}\")\n        print(f\"CUDA version: {torch.version.cuda}\")\n        !nvidia-smi\n        hf_login()\n        print(\"Setting up model and tokenizer...\")\n        model, tokenizer = get_model_and_tokenizer(config.MODEL_NAME)\n        print(\"Preparing dataset...\")\n        dataset = load_and_prepare_dataset(config.DATASET_NAME, tokenizer, config.MAX_SEQ_LENGTH)\n        print(f\"Dataset prepared with splits: {dataset.keys()}\")\n        print(\"Configuring LoRA and training arguments...\")\n        lora_config = get_lora_config()\n        model.gradient_checkpointing_enable()\n        model = prepare_model_for_kbit_training(model)\n        model = get_peft_model(model, lora_config)\n        model.print_trainable_parameters()\n        sft_config = get_sft_config(config)\n        print(\"Starting fine-tuning...\")\n        trainer, live_plotter = fine_tune_model(model, dataset, tokenizer, lora_config, sft_config)\n        \n        print(\"Saving model artifacts...\")\n        save_model_artifacts(trainer, config, sft_config, live_plotter)\n        \n        print(\"Uploading to Hugging Face and generating model card...\")\n        model_card_content, repo_id = upload_to_hf(trainer, config, sft_config)\n        \n        print(\"\\n=== Fine-Tuning and Upload Summary ===\")\n        print(f\"Model card content:\\n{model_card_content}\")\n        print(f\"Model uploaded to: https://huggingface.co/{repo_id}\")\n        print(\"Fine-tuning and upload process completed successfully.\")\n        \n    except Exception as e:\n        print(f\"Error in main loop: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T20:27:08.412494Z","iopub.execute_input":"2025-07-16T20:27:08.412768Z","iopub.status.idle":"2025-07-16T20:27:33.282387Z","shell.execute_reply.started":"2025-07-16T20:27:08.412744Z","shell.execute_reply":"2025-07-16T20:27:33.281179Z"}},"outputs":[{"name":"stdout","text":"Artifacts will be saved to: /kaggle/working/artifacts\nCUDA available: True\nCUDA version: 12.4\nWed Jul 16 20:27:09 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   68C    P0             30W /   70W |    5473MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   43C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\nHugging Face login complete.\nSetting up model and tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1577e4481b04ad893b1178d823be108"}},"metadata":{}},{"name":"stdout","text":"Preparing dataset...\nLoading dataset 'Allanatrix/Scientific_Research_Tokenized'...\nDataset columns: ['input_text', 'target_hypothesis', 'expert_label']\nFormatting and tokenizing dataset with Qwen3 chat template...\nDataset prepared with 4 examples\nDataset prepared with splits: dict_keys(['train'])\nConfiguring LoRA and training arguments...\ntrainable params: 17,432,576 || all params: 1,738,007,552 || trainable%: 1.0030\nStarting fine-tuning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"179cc002f76449c880b325a7b9640ad4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f05259d6194f40528bde7a8da8ab3206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Packing train dataset:   0%|          | 0/4 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccaf2e164ee54941bcef00b16fa81a54"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Starting model fine-tuning...\nTraining started - initializing live plotting...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:05, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Step 2: Loss = 3.4288\nTraining completed!\nSaving model artifacts...\n","output_type":"stream"},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"name":"stdout","text":"Model and tokenizer saved to: /kaggle/working/artifacts/Nexa-Qwen-sci-7B\nTraining configuration saved to: /kaggle/working/artifacts/training_config.json\nTraining arguments saved to: /kaggle/working/artifacts/training_arguments.json\nFinal loss plot saved to: /kaggle/working/artifacts/final_loss_plot.png\nUploading to Hugging Face and generating model card...\nModel card saved to: /kaggle/working/artifacts/Nexa-Qwen-sci-7B/README.md\nFailed to upload to Hugging Face: (Request ID: Root=1-68780b34-55b527c5379aa72e15f990c3;f2e3d661-5cab-4357-9c72-51fc68f99bf3)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"allan-wandia\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.\nError in main loop: (Request ID: Root=1-68780b34-55b527c5379aa72e15f990c3;f2e3d661-5cab-4357-9c72-51fc68f99bf3)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"allan-wandia\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_152/3450234560.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_152/3450234560.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Uploading to Hugging Face and generating model card...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mmodel_card_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload_to_hf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msft_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Fine-Tuning and Upload Summary ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_152/2292029032.py\u001b[0m in \u001b[0;36mupload_to_hf\u001b[0;34m(trainer, config, sft_config)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"allan-wandia/{config.NEW_MODEL_NAME.lower()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         api.upload_folder(\n\u001b[1;32m    336\u001b[0m             \u001b[0mfolder_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3729\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mRepoUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.endpoint}/{repo_type}/{repo_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3730\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3731\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3732\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3733\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3718\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3720\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexist_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nMake sure your token has the correct permissions.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             )\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: Root=1-68780b34-55b527c5379aa72e15f990c3;f2e3d661-5cab-4357-9c72-51fc68f99bf3)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"allan-wandia\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions."],"ename":"HfHubHTTPError","evalue":"(Request ID: Root=1-68780b34-55b527c5379aa72e15f990c3;f2e3d661-5cab-4357-9c72-51fc68f99bf3)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"allan-wandia\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.","output_type":"error"}],"execution_count":15}]}