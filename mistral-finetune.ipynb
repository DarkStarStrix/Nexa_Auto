{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Streamlined Mistral-7B Fine-Tuning for Scientific Research (Reproducible & Structured)\n\nThis notebook is structured to adhere to your requested pattern: all imports at the top,\nfunctions in the middle, and the main execution logic at the bottom.\n\nIt also clarifies the CPU/GPU division for data preparation and model training.\n\n**Execution Flow:**\n* **CPU Phase (Data Preparation/Labeling):** The `load_and_prepare_dataset` function operates on the CPU, handling dataset loading, tokenization, and initial processing.\n* **GPU Phase (Weight Computation):** The `fine_tune_model` function, utilizing `transformers.Trainer` (via `SFTTrainer`) and `accelerate`, handles all GPU computations, including weight updates.\n* **Asynchronous Batching:** `DataCollatorForLanguageModeling` prepares batches on the CPU and efficiently transfers them to the GPU asynchronously during training, managed by the Trainer.\n* **Custom Token Batching (Conceptual):** The \"100M token pool, feed 30M until 100M\" strategy is an advanced data loading pattern. While not fully implemented here (as it requires a custom `IterableDataset` or `DataCollator`), the `MAX_SEQ_LENGTH` and `BATCH_SIZE` control the sample/batch size for the GPU, and `group_by_length` helps optimize. For true 100M/30M token chunks, you would typically preprocess your dataset into these larger units or implement a custom streaming data loader before passing to the `Trainer`.","metadata":{"_uuid":"d74af552-c6f3-434e-8598-3b9175a0db6c","_cell_guid":"799ba8ae-4f6b-43eb-9ed1-ac87ffba468f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-24T15:51:08.502343Z","iopub.status.idle":"2025-06-24T15:51:08.502637Z","shell.execute_reply.started":"2025-06-24T15:51:08.502502Z","shell.execute_reply":"2025-06-24T15:51:08.502517Z"},"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport json\nimport gc\nfrom huggingface_hub import login\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom kaggle_secrets import UserSecretsClient\n\nos.environ[\"BNB_CUDA_VERSION\"] = \"124\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nprint(\"Installing essential libraries...\")\n!pip install --no-deps transformers==4.51.3 bitsandbytes==0.46.0 peft==0.12.0 trl==0.11.1 accelerate==0.34.2\n!pip install datasets\nprint(\"Library installation complete. Please restart your kernel if prompted.\")\ntry:\n    import transformers\n    import bitsandbytes\n    import peft\n    import trl\n    import accelerate\n    print(\"transformers version:\", transformers.__version__)\n    print(\"bitsandbytes version:\", bitsandbytes.__version__)\n    print(\"peft version:\", peft.__version__)\n    print(\"trl version:\", trl.__version__)\n    print(\"accelerate version:\", accelerate.__version__)\n    print(\"torch version:\", torch.__version__)\n    print(f\"CUDA available: {torch.cuda.is_available()}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    !nvidia-smi\nexcept ImportError as e:\n    print(f\"Import error during version check: {e}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T22:55:53.399239Z","iopub.execute_input":"2025-06-24T22:55:53.399566Z","iopub.status.idle":"2025-06-24T22:55:58.355298Z","shell.execute_reply.started":"2025-06-24T22:55:53.399543Z","shell.execute_reply":"2025-06-24T22:55:58.354165Z"}},"outputs":[{"name":"stdout","text":"Installing essential libraries...\nRequirement already satisfied: transformers==4.51.3 in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: bitsandbytes==0.46.0 in /usr/local/lib/python3.11/dist-packages (0.46.0)\nRequirement already satisfied: peft==0.12.0 in /usr/local/lib/python3.11/dist-packages (0.12.0)\nRequirement already satisfied: trl==0.11.1 in /usr/local/lib/python3.11/dist-packages (0.11.1)\nRequirement already satisfied: accelerate==0.34.2 in /usr/local/lib/python3.11/dist-packages (0.34.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nLibrary installation complete. Please restart your kernel if prompted.\ntransformers version: 4.51.3\nbitsandbytes version: 0.46.0\npeft version: 0.12.0\ntrl version: 0.11.1\naccelerate version: 0.34.2\ntorch version: 2.6.0+cu124\nCUDA available: True\nCUDA version: 12.4\nTue Jun 24 22:55:58 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   77C    P0             34W /   70W |    9921MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   72C    P0             31W /   70W |    4207MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Main Functions","metadata":{"execution":{"iopub.status.busy":"2025-06-24T20:16:02.019800Z","iopub.execute_input":"2025-06-24T20:16:02.020118Z","iopub.status.idle":"2025-06-24T20:16:03.656217Z","shell.execute_reply.started":"2025-06-24T20:16:02.020096Z","shell.execute_reply":"2025-06-24T20:16:03.655565Z"}}},{"cell_type":"code","source":"class Config:\n    \"\"\"Centralized configuration for the fine-tuning process.\"\"\"\n    MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n    DATASET_NAME = \"Allanatrix/Scientific_Research_Tokenized\"\n    NEW_MODEL_NAME = \"nexa-mistral-sci7b\"\n    MAX_SEQ_LENGTH = 1024\n    BATCH_SIZE = 1\n    GRADIENT_ACCUMULATION_STEPS = 64\n    LEARNING_RATE = 2e-5\n    NUM_TRAIN_EPOCHS = 2\n    OUTPUT_DIR = \"/kaggle/working/results\"\n    ARTIFACTS_DIR = \"/kaggle/working/artifacts\"\n\n    def to_dict(self):\n        \"\"\"Converts config to a dictionary for JSON export.\"\"\"\n        return {k: v for k, v in vars(self).items() if not k.startswith('__') and not callable(getattr(self, k))}\n\ndef hf_login():\n    \"\"\"Logs into Hugging Face Hub using Kaggle Secrets.\"\"\"\n    try:\n        client = UserSecretsClient()\n        token = client.get_secret(\"HF_TOKEN\")\n        login(token=token)\n        print(\"Hugging Face login complete.\")\n    except Exception as e:\n        print(f\"Failed to access HF_TOKEN: {e}. Please ensure 'HF_TOKEN' is set in Kaggle Secrets.\")\n        raise\n\ndef get_model_and_tokenizer(model_name: str):\n    \"\"\"Loads the base model with 4-bit quantization and its tokenizer.\"\"\"\n    try:\n        torch.cuda.empty_cache()\n        gc.collect()\n        import bitsandbytes as bnb\n        print(\"bitsandbytes loaded successfully\")\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=False,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            trust_remote_code=True,\n            device_map={\"\": 0}\n        )\n        model.config.use_cache = False\n        model.config.pretraining_tp = 1\n        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.padding_side = \"right\"\n        return model, tokenizer\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        print(\"Ensure bitsandbytes is correctly installed and CUDA is compatible.\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\ndef load_and_prepare_dataset(dataset_name: str, tokenizer: AutoTokenizer, max_seq_length: int):\n    \"\"\"Loads and tokenizes the dataset on CPU.\"\"\"\n    print(f\"Loading dataset '{dataset_name}'...\")\n    try:\n        torch.cuda.empty_cache()\n        gc.collect()\n        dataset = load_dataset(dataset_name)\n        print(f\"Dataset columns: {dataset['train'].column_names}\")\n        def tokenize_function(examples):\n            return tokenizer(\n                examples[\"input_text\"],\n                truncation=True,\n                max_length=max_seq_length\n            )\n        print(\"Tokenizing dataset...\")\n        tokenized_dataset = dataset.map(\n            tokenize_function,\n            batched=True,\n            remove_columns=[col for col in dataset[\"train\"].column_names if col != \"input_ids\"],\n            desc=\"Tokenizing dataset\"\n        )\n        tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) > 0, desc=\"Filtering empty sequences\")\n        return tokenized_dataset\n    except Exception as e:\n        print(f\"Error loading or tokenizing dataset: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\ndef get_lora_config():\n    \"\"\"Returns the LoRA configuration.\"\"\"\n    lora_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    return lora_config\n\ndef get_training_arguments(config: Config):\n    \"\"\"Returns the TrainingArguments.\"\"\"\n    training_args = TrainingArguments(\n        output_dir=config.OUTPUT_DIR,\n        num_train_epochs=config.NUM_TRAIN_EPOCHS,\n        per_device_train_batch_size=config.BATCH_SIZE,\n        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n        optim=\"paged_adamw_8bit\",\n        save_steps=25,\n        logging_steps=25,\n        learning_rate=config.LEARNING_RATE,\n        weight_decay=0.001,\n        bf16=True,\n        max_grad_norm=0.3,\n        max_steps=-1,\n        warmup_ratio=0.03,\n        group_by_length=True,\n        lr_scheduler_type=\"cosine\",\n        report_to=\"tensorboard\"\n    )\n    return training_args\n\ndef fine_tune_model(model: AutoModelForCausalLM, dataset, tokenizer: AutoTokenizer, lora_config: LoraConfig, training_args: TrainingArguments, max_seq_length: int):\n    \"\"\"Performs model fine-tuning on GPU.\"\"\"\n    try:\n        torch.cuda.empty_cache()\n        gc.collect()\n        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n        trainer = SFTTrainer(\n            model=model,\n            train_dataset=dataset[\"train\"],\n            peft_config=lora_config,\n            dataset_text_field=\"input_ids\",\n            max_seq_length=max_seq_length,\n            tokenizer=tokenizer,\n            args=training_args\n        )\n        print(\"Starting model fine-tuning...\")\n        trainer.train()\n        return trainer\n    except Exception as e:\n        print(f\"Error during fine-tuning: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\ndef save_model_artifacts(trainer: SFTTrainer, config: Config, training_args: TrainingArguments):\n    \"\"\"Saves the fine-tuned model weights and artifacts.\"\"\"\n    try:\n        final_model_path = os.path.join(config.ARTIFACTS_DIR, config.NEW_MODEL_NAME)\n        trainer.save_model(final_model_path)\n        trainer.tokenizer.save_pretrained(final_model_path)\n        print(f\"Model and tokenizer saved to: {final_model_path}\")\n        config_filename = os.path.join(config.ARTIFACTS_DIR, \"training_config.json\")\n        with open(config_filename, 'w') as f:\n            json.dump(config.to_dict(), f, indent=4)\n        print(f\"Training configuration saved to: {config_filename}\")\n        training_args_filename = os.path.join(config.ARTIFACTS_DIR, \"training_arguments.json\")\n        with open(training_args_filename, 'w') as f:\n            json.dump(training_args.to_dict(), f, indent=4)\n        print(f\"Training arguments saved to: {training_args_filename}\")\n    except Exception as e:\n        print(f\"Error saving artifacts: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T22:55:58.357113Z","iopub.execute_input":"2025-06-24T22:55:58.357462Z","iopub.status.idle":"2025-06-24T22:55:58.377388Z","shell.execute_reply.started":"2025-06-24T22:55:58.357426Z","shell.execute_reply":"2025-06-24T22:55:58.376707Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Main Loop","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"Orchestrates the fine-tuning workflow.\"\"\"\n    try:\n        torch.cuda.empty_cache()\n        gc.collect()\n        config = Config()\n        os.makedirs(config.ARTIFACTS_DIR, exist_ok=True)\n        print(f\"Artifacts will be saved to: {config.ARTIFACTS_DIR}\")\n        print(f\"CUDA available: {torch.cuda.is_available()}\")\n        print(f\"CUDA version: {torch.version.cuda}\")\n        !nvidia-smi\n        hf_login()\n        print(\"Setting up model and tokenizer...\")\n        model, tokenizer = get_model_and_tokenizer(config.MODEL_NAME)\n        print(\"Preparing dataset...\")\n        dataset = load_and_prepare_dataset(config.DATASET_NAME, tokenizer, config.MAX_SEQ_LENGTH)\n        print(f\"Dataset prepared with splits: {dataset.keys()}\")\n        print(\"Configuring LoRA and training arguments...\")\n        lora_config = get_lora_config()\n        model.gradient_checkpointing_enable()\n        model = prepare_model_for_kbit_training(model)\n        model = get_peft_model(model, lora_config)\n        model.print_trainable_parameters()\n        training_args = get_training_arguments(config)\n        print(\"Starting fine-tuning...\")\n        trainer = fine_tune_model(\n            model,\n            dataset,\n            tokenizer,\n            lora_config,\n            training_args,\n            config.MAX_SEQ_LENGTH\n        )\n        print(\"Saving model artifacts...\")\n        save_model_artifacts(trainer, config, training_args)\n        print(\"Fine-tuning complete.\")\n    except Exception as e:\n        print(f\"Error in main loop: {e}\")\n        raise\n    finally:\n        torch.cuda.empty_cache()\n        gc.collect()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T22:55:58.378247Z","iopub.execute_input":"2025-06-24T22:55:58.378566Z","iopub.status.idle":"2025-06-24T22:57:02.111839Z","shell.execute_reply.started":"2025-06-24T22:55:58.378547Z","shell.execute_reply":"2025-06-24T22:57:02.111037Z"}},"outputs":[{"name":"stdout","text":"Artifacts will be saved to: /kaggle/working/artifacts\nCUDA available: True\nCUDA version: 12.4\nTue Jun 24 22:55:59 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   77C    P0             34W /   70W |    9921MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   72C    P0             31W /   70W |    4207MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\nHugging Face login complete.\nSetting up model and tokenizer...\nbitsandbytes loaded successfully\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06e899310df74ff28fc2f0282dd75a1a"}},"metadata":{}},{"name":"stdout","text":"Preparing dataset...\nLoading dataset 'Allanatrix/Scientific_Research_Tokenized'...\nDataset columns: ['input_text', 'target_hypothesis', 'expert_label']\nTokenizing dataset...\nDataset prepared with splits: dict_keys(['train'])\nConfiguring LoRA and training arguments...\ntrainable params: 27,262,976 || all params: 7,268,995,072 || trainable%: 0.3751\nStarting fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Starting model fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:17, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saving model artifacts...\n","output_type":"stream"},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"name":"stdout","text":"Model and tokenizer saved to: /kaggle/working/artifacts/nexa-mistral-sci7b\nTraining configuration saved to: /kaggle/working/artifacts/training_config.json\nTraining arguments saved to: /kaggle/working/artifacts/training_arguments.json\nFine-tuning complete.\n","output_type":"stream"}],"execution_count":27}]}